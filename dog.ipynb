{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/poIlbkNoXept5ql6p+c/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goto0823/dog_cat/blob/main/dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5owuxVev1SMn",
        "outputId": "77ddeb39-c25d-46be-a5d6-9069be38ab18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 223 files belonging to 2 classes.\n",
            "Using 179 files for training.\n",
            "Found 223 files belonging to 2 classes.\n",
            "Using 44 files for validation.\n",
            "Epoch 1/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 166ms/step - accuracy: 0.4437 - loss: 1.0553 - val_accuracy: 0.5682 - val_loss: 0.6909\n",
            "Epoch 2/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 167ms/step - accuracy: 0.4546 - loss: 0.6947 - val_accuracy: 0.5909 - val_loss: 0.6913\n",
            "Epoch 3/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.4160 - loss: 0.6941 - val_accuracy: 0.5455 - val_loss: 0.6920\n",
            "Epoch 4/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 170ms/step - accuracy: 0.5773 - loss: 0.6928 - val_accuracy: 0.5682 - val_loss: 0.6850\n",
            "Epoch 5/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 194ms/step - accuracy: 0.4918 - loss: 0.6953 - val_accuracy: 0.5455 - val_loss: 0.6892\n",
            "Epoch 6/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 263ms/step - accuracy: 0.4854 - loss: 0.6916 - val_accuracy: 0.5682 - val_loss: 0.6742\n",
            "Epoch 7/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 158ms/step - accuracy: 0.5633 - loss: 0.6985 - val_accuracy: 0.5909 - val_loss: 0.6829\n",
            "Epoch 8/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 166ms/step - accuracy: 0.4858 - loss: 0.6975 - val_accuracy: 0.5909 - val_loss: 0.6777\n",
            "Epoch 9/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 182ms/step - accuracy: 0.5041 - loss: 0.6960 - val_accuracy: 0.5682 - val_loss: 0.7142\n",
            "Epoch 10/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 197ms/step - accuracy: 0.4480 - loss: 0.7387 - val_accuracy: 0.5682 - val_loss: 0.6850\n",
            "Epoch 11/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 174ms/step - accuracy: 0.4481 - loss: 0.6920 - val_accuracy: 0.4773 - val_loss: 0.7474\n",
            "Epoch 12/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 186ms/step - accuracy: 0.6526 - loss: 0.6516 - val_accuracy: 0.5682 - val_loss: 0.6882\n",
            "Epoch 13/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 208ms/step - accuracy: 0.5510 - loss: 0.7003 - val_accuracy: 0.4773 - val_loss: 0.6945\n",
            "Epoch 14/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 169ms/step - accuracy: 0.5628 - loss: 0.6901 - val_accuracy: 0.5682 - val_loss: 0.6870\n",
            "Epoch 15/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 207ms/step - accuracy: 0.5156 - loss: 0.6895 - val_accuracy: 0.5455 - val_loss: 0.7089\n",
            "Epoch 16/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 186ms/step - accuracy: 0.6431 - loss: 0.6682 - val_accuracy: 0.5682 - val_loss: 0.6862\n",
            "Epoch 17/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 215ms/step - accuracy: 0.6213 - loss: 0.6451 - val_accuracy: 0.6136 - val_loss: 0.6993\n",
            "Epoch 18/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 168ms/step - accuracy: 0.7307 - loss: 0.5831 - val_accuracy: 0.6591 - val_loss: 0.6737\n",
            "Epoch 19/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 197ms/step - accuracy: 0.6638 - loss: 0.6536 - val_accuracy: 0.6364 - val_loss: 0.6941\n",
            "Epoch 20/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - accuracy: 0.6274 - loss: 0.6611 - val_accuracy: 0.5227 - val_loss: 0.6959\n",
            "Epoch 21/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 168ms/step - accuracy: 0.6760 - loss: 0.5972 - val_accuracy: 0.6136 - val_loss: 0.7202\n",
            "Epoch 22/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 204ms/step - accuracy: 0.7026 - loss: 0.6039 - val_accuracy: 0.6136 - val_loss: 0.7301\n",
            "Epoch 23/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 180ms/step - accuracy: 0.6383 - loss: 0.6337 - val_accuracy: 0.5682 - val_loss: 0.7518\n",
            "Epoch 24/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 170ms/step - accuracy: 0.5329 - loss: 0.7814 - val_accuracy: 0.6818 - val_loss: 0.6997\n",
            "Epoch 25/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 167ms/step - accuracy: 0.6313 - loss: 0.6447 - val_accuracy: 0.5909 - val_loss: 0.7801\n",
            "Epoch 26/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.5999 - loss: 0.6756 - val_accuracy: 0.6364 - val_loss: 0.6913\n",
            "Epoch 27/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 184ms/step - accuracy: 0.6752 - loss: 0.6245 - val_accuracy: 0.5682 - val_loss: 0.7267\n",
            "Epoch 28/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 178ms/step - accuracy: 0.7329 - loss: 0.5376 - val_accuracy: 0.6818 - val_loss: 0.7259\n",
            "Epoch 29/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 197ms/step - accuracy: 0.6490 - loss: 0.6281 - val_accuracy: 0.6591 - val_loss: 0.7419\n",
            "Epoch 30/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 167ms/step - accuracy: 0.6868 - loss: 0.5737 - val_accuracy: 0.6364 - val_loss: 0.7125\n",
            "Epoch 31/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 159ms/step - accuracy: 0.6408 - loss: 0.6019 - val_accuracy: 0.6591 - val_loss: 0.7020\n",
            "Epoch 32/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 171ms/step - accuracy: 0.6506 - loss: 0.6244 - val_accuracy: 0.6591 - val_loss: 0.6947\n",
            "Epoch 33/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 202ms/step - accuracy: 0.7265 - loss: 0.5873 - val_accuracy: 0.6591 - val_loss: 0.7614\n",
            "Epoch 34/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 198ms/step - accuracy: 0.6809 - loss: 0.6036 - val_accuracy: 0.6591 - val_loss: 0.7488\n",
            "Epoch 35/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.6830 - loss: 0.6003 - val_accuracy: 0.6364 - val_loss: 0.7729\n",
            "Epoch 36/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 204ms/step - accuracy: 0.7262 - loss: 0.5630 - val_accuracy: 0.6818 - val_loss: 0.7184\n",
            "Epoch 37/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 188ms/step - accuracy: 0.7766 - loss: 0.5045 - val_accuracy: 0.6364 - val_loss: 0.8052\n",
            "Epoch 38/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 170ms/step - accuracy: 0.8030 - loss: 0.4981 - val_accuracy: 0.7045 - val_loss: 0.7542\n",
            "Epoch 39/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 169ms/step - accuracy: 0.7119 - loss: 0.5059 - val_accuracy: 0.6591 - val_loss: 0.7390\n",
            "Epoch 40/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 166ms/step - accuracy: 0.7608 - loss: 0.5007 - val_accuracy: 0.6591 - val_loss: 0.7512\n",
            "Epoch 41/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 241ms/step - accuracy: 0.7199 - loss: 0.5324 - val_accuracy: 0.5909 - val_loss: 0.7704\n",
            "Epoch 42/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 167ms/step - accuracy: 0.7344 - loss: 0.5340 - val_accuracy: 0.6364 - val_loss: 0.8214\n",
            "Epoch 43/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 171ms/step - accuracy: 0.7796 - loss: 0.4781 - val_accuracy: 0.5682 - val_loss: 0.8645\n",
            "Epoch 44/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 203ms/step - accuracy: 0.7661 - loss: 0.4912 - val_accuracy: 0.7273 - val_loss: 0.6868\n",
            "Epoch 45/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 185ms/step - accuracy: 0.7759 - loss: 0.4623 - val_accuracy: 0.7045 - val_loss: 0.7082\n",
            "Epoch 46/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 168ms/step - accuracy: 0.7858 - loss: 0.4264 - val_accuracy: 0.6364 - val_loss: 0.8805\n",
            "Epoch 47/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 157ms/step - accuracy: 0.7971 - loss: 0.4525 - val_accuracy: 0.5682 - val_loss: 0.7295\n",
            "Epoch 48/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 166ms/step - accuracy: 0.7875 - loss: 0.5011 - val_accuracy: 0.6818 - val_loss: 0.8300\n",
            "Epoch 49/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 190ms/step - accuracy: 0.7931 - loss: 0.4100 - val_accuracy: 0.6136 - val_loss: 0.9185\n",
            "Epoch 50/50\n",
            "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 168ms/step - accuracy: 0.8132 - loss: 0.4312 - val_accuracy: 0.5682 - val_loss: 0.8838\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
            "この画像は 96.28 %の確率で dog であり、3.72 %の確率で not_dog です。\n",
            "最終的な判定: この画像は dog です！\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "data_dir = \"/content/detasets/\"\n",
        "\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH  = 150\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=4 # Set validation batch size to the size of the validation dataset\n",
        ")\n",
        "\n",
        "data_arg = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  data_arg,\n",
        "  # 入力層（入力の形と、ピクセル値の正規化）\n",
        "  tf.keras.layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "\n",
        "  # 畳み込みブロック1\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "  # 畳み込みブロック2\n",
        "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "  # 畳み込みブロック3\n",
        "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "  # 最後の分類のための層\n",
        "  tf.keras.layers.Flatten(), # ここで初めて平坦化する\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid') # 最終的な出力\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# 1. 画像を読み込み、指定したサイズにリサイズする\n",
        "img = tf.keras.utils.load_img(\n",
        "    \"/content/data/cat2.jpeg\", target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        ")\n",
        "# 2. 画像を数値の配列(NumPy array)に変換する\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "# 3. モデルは一度に複数の画像を処理する「バッチ」形式でデータを期待するため、\n",
        "#    次元を一つ追加して (1, 150, 150, 3) という形にする\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "# 4. モデルに画像データを渡して予測を実行！\n",
        "predictions = model.predict(img_array)\n",
        "score = predictions[0][0] # 予測結果は [[...]] のような形で返ってくるので、中の数値を取り出す\n",
        "# 5. 結果を人間が分かりやすいように表示する\n",
        "class_names = train_dataset.class_names # 学習データからクラス名['dog', 'not_dog']を取得\n",
        "print(\n",
        "    \"この画像は {:.2f} %の確率で {} であり、{:.2f} %の確率で {} です。\"\n",
        "    .format(100 * (1 - score), class_names[0], 100 * score, class_names[1])\n",
        ")\n",
        "# 閾値（例えば0.5）を使って、最終的な判定を下す\n",
        "if score < 0.5:\n",
        "    print(f\"最終的な判定: この画像は {class_names[0]} です！\")\n",
        "else:\n",
        "    print(f\"最終的な判定: この画像は {class_names[1]} です！\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2752eb22",
        "outputId": "08f771e1-e0b8-467e-dccf-1591a53e71e8"
      },
      "source": [
        "import os\n",
        "\n",
        "data_dir = \"/content/datasets/\"\n",
        "\n",
        "# Create the main directory\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# Create dummy class subdirectories\n",
        "class_names = [\"cat\", \"dog\"]\n",
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(data_dir, class_name), exist_ok=True)\n",
        "\n",
        "# Create dummy image files (replace with your actual images)\n",
        "# For demonstration, we'll just create empty files\n",
        "for i in range(5):\n",
        "    with open(os.path.join(data_dir, \"cat\", f\"cat_{i}.jpg\"), \"w\") as f:\n",
        "        pass\n",
        "    with open(os.path.join(data_dir, \"dog\", f\"dog_{i}.jpg\"), \"w\") as f:\n",
        "        pass\n",
        "\n",
        "print(f\"Dummy dataset structure created in {data_dir}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy dataset structure created in /content/datasets/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "2f83c654",
        "outputId": "52b0642d-b9b5-4db4-8bd4-edb5d6c3d48d"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "data_dir = \"/content/datasets/\"\n",
        "\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH  = 150\n",
        "\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=4 # Set validation batch size to the size of the validation dataset\n",
        ")\n",
        "\n",
        "data_arg = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  data_arg,\n",
        "  # 入力層（入力の形と、ピクセル値の正規化）\n",
        "  tf.keras.layers.Rescaling(1./255, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "\n",
        "  # 畳み込みブロック1\n",
        "  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "  # 畳み込みブロック2\n",
        "  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "  # 畳み込みブロック3\n",
        "  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "  # 最後の分類のための層\n",
        "  tf.keras.layers.Flatten(), # ここで初めて平坦化する\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid') # 最終的な出力\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# 1. 画像を読み込み、指定したサイズにリサイズする\n",
        "img = tf.keras.utils.load_img(\n",
        "    \"/content/datasets/cat/cat_0.jpg\", target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
        ")\n",
        "# 2. 画像を数値の配列(NumPy array)に変換する\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "# 3. モデルは一度に複数の画像を処理する「バッチ」形式でデータを期待するため、\n",
        "#    次元を一つ追加して (1, 150, 150, 3) という形にする\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "# 4. モデルに画像データを渡して予測を実行！\n",
        "predictions = model.predict(img_array)\n",
        "score = predictions[0][0] # 予測結果は [[...]] のような形で返ってくるので、中の数値を取り出す\n",
        "# 5. 結果を人間が分かりやすいように表示する\n",
        "class_names = train_dataset.class_names # 学習データからクラス名['dog', 'not_dog']を取得\n",
        "print(\n",
        "    \"この画像は {:.2f} %の確率で {} であり、{:.2f} %の確率で {} です。\"\n",
        "    .format(100 * (1 - score), class_names[0], 100 * score, class_names[1])\n",
        ")\n",
        "# 閾値（例えば0.5）を使って、最終的な判定を下す\n",
        "if score < 0.5:\n",
        "    print(f\"最終的な判定: この画像は {class_names[0]} です！\")\n",
        "else:\n",
        "    print(f\"最終的な判定: この画像は {class_names[1]} です！\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 files belonging to 2 classes.\n",
            "Using 8 files for training.\n",
            "Found 10 files belonging to 2 classes.\n",
            "Using 2 files for validation.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nInput is empty.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_93511]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12441009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m               )\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nInput is empty.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_93511]"
          ]
        }
      ]
    }
  ]
}